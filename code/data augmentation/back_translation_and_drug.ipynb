{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beab38f-9235-4e2f-9255-407b0a12a330",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyarrow\n",
    "!pip install BackTranslation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ac864c3-f8eb-47d9-9029-005945f3079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from BackTranslation import BackTranslation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc16c01c-50e7-4798-86cd-707b5adc03a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: https://ansm.sante.fr/documents/reference/repertoire-des-medicaments-generiques\n",
    "#create drugs.txt\n",
    "\n",
    "res = []\n",
    "with open(\"data/liste_medicaments.txt\", encoding = 'utf-8') as f:\n",
    "    for line in f:\n",
    "        if len(line) >= 22 and line[:22] == 'Dénomination commune :':\n",
    "            line = line[23:]\n",
    "            n_medicament = line.split(' Voie')[0]\n",
    "            n_medicament = re.sub('\\(.*?\\)', '', n_medicament)\n",
    "            n_medicament = n_medicament.split('+')\n",
    "            for medicament in n_medicament:\n",
    "                medicament = \"\".join([x if (x.isalpha() or x.isspace()) else '' for x in medicament]).strip().lower()\n",
    "                medicament = \" \".join(list(dict.fromkeys(medicament.split(\" \"))))\n",
    "                res.append(medicament)\n",
    "medic_list_propre = pd.DataFrame(set(res), columns = [\"medicament\"])\n",
    "medic_list_propre.to_csv(\"data/drugs.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78236c4b-c0a4-4004-a350-f145d32961c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error with: une \n",
      "error with: une \n",
      "error with: une \n",
      "error with: une \n",
      "error with: une \n",
      "error with: une \n",
      "error with: manquants. \n",
      "error with: une \n",
      "error with: une \n",
      "error with: irrégulières \n"
     ]
    }
   ],
   "source": [
    "#create back_translation dictionnaries\n",
    "\n",
    "import time\n",
    "from collections import defaultdict\n",
    "df = pd.read_parquet(\"data/tokens_sans_bert.parquet\")\n",
    "trans = BackTranslation(url=[\n",
    "      'translate.google.com',\n",
    "      'translate.google.co.kr',\n",
    "    ], proxies={'http': '127.0.0.1:1234', 'http://host.name': '127.0.0.1:4012'})\n",
    "id_add = 101\n",
    "df_base_sentence = df[df[\"id_phrase\"] % 1000 == 0]\n",
    "dict_sentence = defaultdict(lambda : \"\")\n",
    "dict_labels = defaultdict(lambda : [])\n",
    "cur_phrase_part = \"\"\n",
    "cur_phrase_translated = \"\"\n",
    "cur_id_phrase = 1000\n",
    "for index, row in df_base_sentence.iterrows():\n",
    "    if row[\"label\"] == 'O' and row[\"id_phrase\"] == cur_id_phrase:\n",
    "        cur_phrase_part += row[\"token\"] + \" \"\n",
    "    else:\n",
    "        if cur_phrase_part != \"\":\n",
    "            try:\n",
    "                time.sleep(2)\n",
    "                cur_phrase_translated += trans.translate(cur_phrase_part, src='fr', tmp='en').result_text\n",
    "                cur_phrase_part = \"\"\n",
    "            except:\n",
    "                print(\"error with: \" + cur_phrase_part)\n",
    "        if row[\"id_phrase\"] != cur_id_phrase:\n",
    "            dict_sentence[cur_id_phrase + id_add] = cur_phrase_translated + \" \"\n",
    "            cur_id_phrase = row[\"id_phrase\"]\n",
    "            cur_phrase_translated = \"\"\n",
    "        if row[\"label\"] != 'O':\n",
    "            dict_labels[cur_id_phrase + id_add].append([row['label'].split('-')[1], len(cur_phrase_translated), len(cur_phrase_translated) + len(row[\"token\"])])\n",
    "            cur_phrase_translated += row[\"token\"] + \" \"\n",
    "        else:\n",
    "            cur_phrase_part += row[\"token\"]\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd957ec-0d6e-415d-8e53-156353cd89f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create back_translation dataset\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"data/back_translated.jsonl\", 'w', encoding='utf8') as f:\n",
    "    for id_phrase in dict_sentence:\n",
    "        f.write(json.dumps({\"id\": id_phrase, \"text\": dict_sentence[id_phrase], \"labels\": dict_labels[id_phrase], \"Comments\": []}, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f562a789-85de-49fe-9bc2-6efcee70ac7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
